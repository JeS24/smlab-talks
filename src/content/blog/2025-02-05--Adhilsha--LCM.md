---
author: Adhilsha Ansad
pubDatetime: 2025-02-05T19:00:00.000+05:30
modDatetime: 
title: Large Concept Models
featured: false
draft: false
slug: LCM
tags:
  - "2025"
  - "Large Concept Models"
  - "Sentence level tokenization"
  - "Transformer"
  - "Diffusion Model"
  - "Quantization"
description: This talk will will explore the training objectives, segmentation techniques, and generation strategies of Large Concept Models (LCMs). LCMs are a novel class of models that leverage sentence-level tokenization to represent concepts, a higher abstraction than current tokens. We will also discuss the quantization of LCMs and their potential applications in various domains.
---

Large Concept Models (LCMs) are a novel approach to language modeling that operates in a sentence representation space rather than token-level processing. We will begin by discussing how LCMs abstract information hierarchically, processing concepts instead of raw text or speech. We will outline the architectures and training objectives, along with data preparation and segmentation techniques that enable LCMs to capture complex relationships between concepts. We will then explore three key LCM variants: Base-LCM, Diffusion-LCM, and Quantized-LCM. We will also discuss the potential applications of LCMs and limitations that leads to future research directions.

Additional resources:
* LCM paper - https://arxiv.org/abs/2412.08821
* SONAR paper - https://arxiv.org/abs/2308.11466
* Diffusion Models - https://erdem.pl/2023/11/step-by-step-visual-introduction-to-diffusion-models

<embed src="/labtalks/assets/slides/2025-02-05--Adhilsha--LCM.pdf" type="application/pdf" width="100%" height="600px">
