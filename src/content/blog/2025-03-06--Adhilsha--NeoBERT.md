---
author: Adhilsha Ansad
pubDatetime: 2025-03-06T08:00:00.000+05:30 
modDatetime: 
title: NeoBERT
featured: false
draft: false
slug: NeoBERT
tags:
  - "2025" 
  - "NLP"
  - "BERT"
  - "NeoBERT"
  - "short-talks"
  - "transformers"
description: In this talk, I introduce NeoBERT, a next-generation encoder that redefines bidirectional modeling through state-of-the-art architectural improvements, modern data strategies, and optimized pre-training. 
---

Recent advancements in large language models have pushed the boundaries of in-context learning and reasoning, yet bidirectional encoders like BERT have lagged behind in innovation. In this talk, I introduce NeoBERT, a next-generation encoder that redefines bidirectional modeling through state-of-the-art architectural improvements, modern data strategies, and optimized pre-training. I will highlight key enhancements, including an optimal depth-to-width ratio, an extended 4,096-token context length, and efficiency-driven modifications that enable NeoBERT to achieve state-of-the-art results with just 250M parameters. 

Additional resources:
* https://arxiv.org/abs/2502.19587 - NeoBERT paper
* https://www.analyticsvidhya.com/blog/2019/09/demystifying-bert-groundbreaking-nlp-framework/ - Some terminologies in BERT


<embed src="/labtalks/assets/slides/2025-03-06--Adhilsha--NeoBERT.pdf" type="application/pdf" width="100%" height="600px">
